# -*- coding: utf-8 -*-
"""ProyekPBA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vPV_hViq4QRn0zpAo_8eYYtBC2Fr1aYl

#Importing Library
"""

pip install --user tensorflow==1.0.0

!pip install deeppavlov

# Import Libraries

import numpy as np 
import pandas as pd
from tqdm import tqdm
import re
import nltk
from nltk.corpus import stopwords

# deeppavlov
from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader
from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator
from deeppavlov.models.preprocessors.str_lower import str_lower
from deeppavlov.models.tokenizers.nltk_moses_tokenizer import NLTKMosesTokenizer
from deeppavlov.core.data.simple_vocab import SimpleVocabulary
from deeppavlov.models.embedders.bow_embedder import BoWEmbedder
from deeppavlov.core.data.utils import simple_download
from deeppavlov.models.embedders.glove_embedder import GloVeEmbedder
from deeppavlov.metrics.accuracy import sets_accuracy
from deeppavlov.models.classifiers.keras_classification_model import KerasClassificationModel
from deeppavlov.models.preprocessors.one_hotter import OneHotter
from deeppavlov.models.classifiers.proba2labels import Proba2Labels

# import os

# Any results you write to the current directory are saved as output.

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/My\ Drive

# Commented out IPython magic to ensure Python compatibility.
# %cd Colab\ Notebooks

!ls

"""#Data Load"""

data = pd.read_csv('PBA.csv')
data

# read data from particular columns of `.csv` file
dr = BasicClassificationDatasetReader().read(
    data_path='./',
    train='PBA.csv',
    x = 'short_description',
    y = 'category'
)

# initialize data iterator splitting `train` field to `train` and `valid` in proportion 0.8/0.2
train_iterator = BasicClassificationDatasetIterator(
    data=dr,
    field_to_split='train',  # field that will be splitted
    split_fields=['train', 'valid'],   # fields to which the fiald above will be splitted
    split_proportions=[0.8, 0.2],  #proportions for splitting
    split_seed=23,  # seed for splitting dataset
    seed=42)  # seed for iteration over dataset

# one can get train instances (or any other data type including `all`)
x_train, y_train = train_iterator.get_instances(data_type='train')
for x, y in list(zip(x_train, y_train))[:10]:
    print('x:', x)
    print('y:', y)
    print('=================')

"""#Data Preprocessing"""

data['short_description'] = data['short_description'].str.replace('[^\w\s]', '')

print(data)

data['short_description'] = data['short_description'].str.lower()

print(data)

from nltk import PorterStemmer

porter = PorterStemmer()

def stemmer(sentence):
    tokens = sentence.split()
    stemmed_tokens = [porter.stem(token) for token in tokens]
    return ' '.join(stemmed_tokens)

data['short_description'] = data['short_description'].apply(stemmer)

print(data)

from nltk import WordNetLemmatizer

import nltk
nltk.download('wordnet')

wordNet = WordNetLemmatizer()

def lemmatizer(sentence):
    tokens = sentence.split()
    lemmatized_tokens = [wordNet.lemmatize(token) for token in tokens]
    return ' '.join(lemmatized_tokens)

data['short_description'] = data['short_description'].apply(lemmatizer)

print(data)

from sklearn.feature_extraction.text import CountVectorizer

unigrams = CountVectorizer(ngram_range=(1, 1), analyzer='word')
bigrams = CountVectorizer(ngram_range=(2, 2), analyzer='word')
trigrams = CountVectorizer(ngram_range=(3, 3), analyzer='word')
fourgrams = CountVectorizer(ngram_range=(4, 4), analyzer='word')

unigramsDescriptionData = unigrams.fit_transform(data['short_description'])
frequencies = sum(unigramsDescriptionData).data
hasil = pd.DataFrame(frequencies, index=unigrams.get_feature_names(), columns=['frequency'])

print(hasil)

bigramsDescriptionData = bigrams.fit_transform(data['short_description'])
frequencies = sum(bigramsDescriptionData).data
hasil = pd.DataFrame(frequencies, index=bigrams.get_feature_names(), columns=['frequency'])

print(hasil)

trigramsDescriptionData = trigrams.fit_transform(data['short_description'])
frequencies = sum(trigramsDescriptionData).data
hasil = pd.DataFrame(frequencies, index=trigrams.get_feature_names(), columns=['frequency'])

print(hasil)

fourgramsDescriptionData = fourgrams.fit_transform(data['short_description'])
frequencies = sum(fourgramsDescriptionData).data
hasil = pd.DataFrame(frequencies, index=fourgrams.get_feature_names(), columns=['frequency'])

print(hasil)

from sklearn.feature_extraction.text import TfidfTransformer

tfidfer = TfidfTransformer()

unigramsDescriptionData = unigrams.fit_transform(data['category'])
frequencies = sum(unigramsDescriptionData).data
tfidfDataDescription = tfidfer.fit_transform(unigramsDescriptionData)
results = sum(tfidfDataDescription).data
dataDescription = pd.DataFrame(frequencies, index=unigrams.get_feature_names(), columns=['frequency'])
dataDescription['result'] = results

print(dataDescription)

from sklearn.model_selection import train_test_split

point,description = data['category'], data['short_description']
pointTrain,pointTest,descriptionTrain,descriptionTest = train_test_split(data['category'],
                                                                         data['short_description'],
                                                                         test_size = 0.2,
                                                                         random_state=0)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

train_set = data.sample(frac=0.8,random_state=200)
test_set = data.drop(train_set.index)

count_vect = CountVectorizer()
tfidf_transformer = TfidfTransformer()

X_train_tf = count_vect.fit_transform(data['short_description'])
X_train_tfidf = tfidf_transformer.fit_transform(X_train_tf)

mod = MultinomialNB()
mod.fit(X_train_tfidf, data['category'])
print(mod)

X_test_tf = count_vect.transform(data['short_description'])
X_test_tfidf = tfidf_transformer.transform(X_test_tf)

predicted = mod.predict(X_test_tfidf)
print("Accuracy: ", accuracy_score(data['category'], predicted))

from sklearn.metrics import classification_report

print(classification_report(data['category'],
                           predicted))

"""##Lowercasing"""

# str_lower = s()
# check
str_lower(['Kaggle is the best place to study machine learning.'])

"""##Tokenizer"""

tokenizer = NLTKMosesTokenizer()
# check
tokenizer(['Kaggle is the best place to study machine learning.'])

train_x_lower_tokenized = str_lower(tokenizer(train_iterator.get_instances(data_type='train')[0]))

"""##Vocabulary"""

# initialize simple vocabulary to collect all appeared in the dataset classes
classes_vocab = SimpleVocabulary(
    save_path='./tmp/classes.dict',
    load_path='./tmp/classes.dict')

classes_vocab.fit((train_iterator.get_instances(data_type='train')[1]))
classes_vocab.save()

# show classes
list(classes_vocab.items())

# also one can collect vocabulary of textual tokens appeared 2 and more times in the dataset
token_vocab = SimpleVocabulary(
    save_path='./tmp/tokens.dict',
    load_path='./tmp/tokens.dict',
    min_freq=2,
    special_tokens=('<PAD>', '<UNK>',),
    unk_token='<UNK>')

token_vocab.fit(train_x_lower_tokenized)
token_vocab.save()

# number of tokens in dictionary
len(token_vocab)

# 10 most common words and number of times their appeared
token_vocab.freqs.most_common()[:15]

"""##Bag-of-Words"""

# initialize bag-of-words embedder giving total number of tokens
bow = BoWEmbedder(depth=token_vocab.len)
# it assumes indexed tokenized samples
bow(token_vocab(str_lower(tokenizer(['Kaggle is the best place to study machine learning.']))))

# all 10 tokens are in the vocabulary
sum(bow(token_vocab(str_lower(tokenizer(['Kaggle is the best place to study machine learning.']))))[0])

"""##GloVe Embedder"""

# Glove : https://nlp.stanford.edu/projects/glove/
simple_download(url="http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt", destination="./glove.6B.100d.txt")

embedder = GloVeEmbedder(load_path='./glove.6B.100d.txt',dim=100, pad_zero=True)

"""#Model Build"""

# get all train and valid data from iterator
x_train, y_train = train_iterator.get_instances(data_type="train")
x_valid, y_valid = train_iterator.get_instances(data_type="valid")

# Intialize `KerasClassificationModel` that composes CNN shallow-and-wide network 
# (name here as`cnn_model`)
cls = KerasClassificationModel(save_path="./cnn_model_v0", 
                               load_path="./cnn_model_v0", 
                               embedding_size=embedder.dim,
                               n_classes=classes_vocab.len,
                               model_name="cnn_model",
                               text_size=15, # number of tokens
                               kernel_sizes_cnn=[3, 5, 7],
                               filters_cnn=128,
                               dense_size=100,
                               optimizer="Adam",
                               learning_rate=0.1,
                               learning_rate_decay=0.01,
                               loss="categorical_crossentropy")

onehotter = OneHotter(depth=classes_vocab.len, single_vector=True)

"""#Train"""

for ep in range(10):
    for x, y in tqdm(train_iterator.gen_batches(batch_size=64, 
                                           data_type="train")):
        x_embed = embedder(tokenizer(str_lower(x)))
        y_onehot = onehotter(classes_vocab(y))
        cls.train_on_batch(x_embed, y_onehot)

cls.save()

"""#Check Result"""

# Infering on validation data we get probability distribution on given data.
y_valid_pred = cls(embedder(tokenizer(str_lower(x_valid))))

prob2labels = Proba2Labels(max_proba=True)

# Let's look into obtained result
print("Text sample: {}".format(x_valid[10]))
print("True label: {}".format(y_valid[10]))
print("Predicted probability distribution: {}".format(dict(zip(classes_vocab.keys(), 
                                                               y_valid_pred[10]))))
print("Predicted label: {}".format(classes_vocab(prob2labels(y_valid_pred))[10]))

# calculate sets accuracy
sets_accuracy(y_valid, classes_vocab(prob2labels(y_valid_pred)))

